{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROVIDEDH Collaborative platform\n",
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from functools import reduce\n",
    "from lxml import etree as et\n",
    "from lxml.etree import Element\n",
    "import itertools\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dep_folder = './Version 8.3.10 with original normalized depositions + marked persons/'\n",
    "seed_dep_folder = './seed/'\n",
    "processed_dep_folder = './martes'#'./Version 8.3.10 with original normalized depositions + marked persons + gazeette ner/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 File Processing\n",
    "\n",
    "File procesing for retrieving a  list of non annotated text tokens along with the\n",
    "information regarding the DOM context.\n",
    "\n",
    "1. Preprocess the document\n",
    "    1. Cleanup of embbeded characters:\n",
    "        - __`&#13`__ -> __`<br/>`__\n",
    "        - __`&gt`__ -> __`>`__\n",
    "        - __`&lt`__ -> __`<`__\n",
    "\n",
    "    2. Creation of XML DOM tree\n",
    "2. Processing of the document\n",
    "    3. Extraction of non empty text nodes\n",
    "    4. Recursively assign parent tag types\n",
    "    5. Filter out already annotated text nodes\n",
    "    6. Split text nodes into word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = {\n",
    "    'tei': 'http://www.tei-c.org/ns/1.0',\n",
    "    'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "    'xi': 'http://www.w3.org/2001/XInclude',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_annotation_list(dep_raw):\n",
    "    dep_tree = et.fromstring(dep_raw.encode())\n",
    "\n",
    "    certainties = dep_tree.xpath('//tei:teiHeader'\n",
    "                                 '//tei:classCode[@scheme=\"http://providedh.eu/uncertainty/ns/1.0\"]',\n",
    "                                 namespaces=namespaces)\n",
    "\n",
    "    if not certainties:\n",
    "        add_annotation_list(dep_tree)\n",
    "\n",
    "    text = et.tounicode(dep_tree)\n",
    "\n",
    "    return text\n",
    "\n",
    "def add_annotation_list(dep_tree):\n",
    "    default_namespace = namespaces['tei']\n",
    "    default = \"{%s}\" % default_namespace\n",
    "\n",
    "    ns_map = {\n",
    "        None: default_namespace\n",
    "    }\n",
    "\n",
    "    profile_desc = dep_tree.xpath('//tei:teiHeader/tei:profileDesc', namespaces=namespaces)\n",
    "\n",
    "    if not profile_desc:\n",
    "        tei_header = dep_tree.xpath('//tei:teiHeader', namespaces=namespaces)\n",
    "        profile_desc = et.Element(default + 'profileDesc', nsmap=ns_map)\n",
    "        tei_header[0].append(profile_desc)\n",
    "\n",
    "    text_class = dep_tree.xpath('//tei:teiHeader/tei:profileDesc/tei:textClass', namespaces=namespaces)\n",
    "\n",
    "    if not text_class:\n",
    "        profile_desc = dep_tree.xpath('//tei:teiHeader/tei:profileDesc', namespaces=namespaces)\n",
    "        text_class = et.Element(default + 'textClass', nsmap=ns_map)\n",
    "        profile_desc[0].append(text_class)\n",
    "\n",
    "    class_code = dep_tree.xpath(\n",
    "        '//tei:teiHeader/tei:profileDesc/tei:textClass/tei:classCode[@scheme=\"http://providedh.eu/uncertainty/ns/1.0\"]',\n",
    "        namespaces=namespaces)\n",
    "\n",
    "    if not class_code:\n",
    "        text_class = dep_tree.xpath('//tei:teiHeader/tei:profileDesc/tei:textClass', namespaces=namespaces)\n",
    "        class_code = et.Element(default + 'classCode', scheme=\"http://providedh.eu/uncertainty/ns/1.0\",\n",
    "                                   nsmap=ns_map)\n",
    "        text_class[0].append(class_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotator(dep_raw):\n",
    "    dep_tree = et.fromstring(dep_raw.encode())\n",
    "\n",
    "    list_person = dep_tree.xpath('//tei:teiHeader'\n",
    "                             '//tei:listPerson[@type=\"PROVIDEDH Annotators\"]', namespaces=namespaces)\n",
    "\n",
    "    if not list_person:\n",
    "        dep_tree = create_list_person(dep_tree)\n",
    "        list_person = dep_tree.xpath('//tei:teiHeader'\n",
    "                                 '//tei:listPerson[@type=\"PROVIDEDH Annotators\"]', namespaces=namespaces)\n",
    "\n",
    "    annotators = dep_tree.xpath('//tei:teiHeader'\n",
    "                                '//tei:listPerson[@type=\"PROVIDEDH Annotators\"]'\n",
    "                                '/tei:person', namespaces=namespaces)\n",
    "\n",
    "    xml_ids = []\n",
    "    for annotator in annotators:\n",
    "        prefix = '{%s}' % namespaces['xml']\n",
    "        xml_id = annotator.get(prefix + 'id')\n",
    "\n",
    "        xml_ids.append(xml_id)\n",
    "        \n",
    "    if 'automatic_gazeetter_ner' not in xml_ids:\n",
    "        list_person[0].append(et.fromstring(\n",
    "            u\"\"\"\n",
    "                <person xml:id=\"automatic_gazeetter_ner\">\n",
    "                  <persName>\n",
    "                    <forename>Gazeette based NER</forename>\n",
    "                    <surname>none</surname>\n",
    "                    <email>none</email>\n",
    "                  </persName>\n",
    "                  <link>none</link>\n",
    "                </person>\n",
    "            \"\"\"\n",
    "        ))\n",
    "\n",
    "    text = et.tounicode(dep_tree)\n",
    "\n",
    "    return text\n",
    "\n",
    "def create_list_person(dep_tree):\n",
    "    prefix = \"{%s}\" % namespaces['tei']\n",
    "\n",
    "    ns_map = {\n",
    "        None: namespaces['tei']\n",
    "    }\n",
    "\n",
    "    profile_desc = dep_tree.xpath('//tei:teiHeader/tei:profileDesc', namespaces=namespaces)\n",
    "\n",
    "    if not profile_desc:\n",
    "        tei_header = dep_tree.xpath('//tei:teiHeader', namespaces=namespaces)\n",
    "        profile_desc = et.Element(prefix + 'profileDesc', nsmap=ns_map)\n",
    "        tei_header[0].append(profile_desc)\n",
    "\n",
    "    partic_desc = dep_tree.xpath('//tei:teiHeader/tei:profileDesc/tei:particDesc', namespaces=namespaces)\n",
    "\n",
    "    if not partic_desc:\n",
    "        profile_desc = dep_tree.xpath('//tei:teiHeader/tei:profileDesc', namespaces=namespaces)\n",
    "        partic_desc = et.Element(prefix + 'particDesc', nsmap=ns_map)\n",
    "        profile_desc[0].append(partic_desc)\n",
    "\n",
    "    list_person = dep_tree.xpath(\n",
    "        '//tei:teiHeader/tei:profileDesc/tei:particDesc/tei:listPerson[@type=\"PROVIDEDH Annotators\"]',\n",
    "        namespaces=namespaces)\n",
    "\n",
    "    if not list_person:\n",
    "        partic_desc = dep_tree.xpath('//tei:teiHeader/tei:profileDesc/tei:particDesc',\n",
    "                                 namespaces=namespaces)\n",
    "        list_person = et.Element(prefix + 'listPerson', type=\"PROVIDEDH Annotators\", nsmap=ns_map)\n",
    "        partic_desc[0].append(list_person)\n",
    "\n",
    "    return dep_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_cleanup(file_text):\n",
    "    clean_carriege_return = lambda s: s.replace('&#13;','<br/>')\n",
    "    clean_carriege_return2 = lambda s: s.replace('#xd;','<br/>')\n",
    "    clean_greater_than = lambda s: s.replace('&gt;','\\>')\n",
    "    clean_lesser_than = lambda s: s.replace('&lt;','\\<')\n",
    "    \n",
    "    preprocessing = [\n",
    "        clean_carriege_return, \n",
    "        clean_carriege_return2,\n",
    "        #clean_greater_than, \n",
    "        #clean_lesser_than,\n",
    "    ]\n",
    "    \n",
    "    return reduce(lambda x,f: f(x), preprocessing, file_text)\n",
    "\n",
    "def create_lxml_tree(file_text):\n",
    "    return et.fromstring(file_text.encode())\n",
    "\n",
    "def preprocess(file_text):\n",
    "    preprocessing = (\n",
    "        character_cleanup,\n",
    "        add_annotator,\n",
    "        setup_annotation_list,\n",
    "        create_lxml_tree\n",
    "    )\n",
    "\n",
    "    return reduce(lambda x,f: f(x), preprocessing, file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namescpaces should be set in a previous cell\n",
    "# namespaces = {'tei': 'http://www.tei-c.org/ns/1.0', 'xml': 'http://www.w3.org/XML/1998/namespace'}\n",
    "def get_body_text_nodes(tree_root, namespaces=namespaces):\n",
    "    \"\"\"Retrieve all text nodes in the body of lxml tree.\"\"\"\n",
    "\n",
    "    body = tree_root.find('.//tei:body', namespaces=namespaces)\n",
    "    return body.xpath('.//text()')\n",
    "\n",
    "def filter_out_empty_text_nodes(text_nodes):\n",
    "    not_empty = lambda node: node.strip() != ''\n",
    "    return filter(not_empty, text_nodes)\n",
    "\n",
    "def remove_extra_spaces(text_nodes):\n",
    "    inner_spaces = re.compile('[ \\n\\r]{2,}')\n",
    "    tree_root = None\n",
    "\n",
    "    for node in text_nodes:\n",
    "        if tree_root is None:\n",
    "            tree_root = node.getparent().getroottree()\n",
    "\n",
    "        trimmed_text= inner_spaces.sub(' ', node).strip()\n",
    "\n",
    "        if node.is_text:\n",
    "            node.getparent().text = trimmed_text\n",
    "        elif node.is_tail:\n",
    "            node.getparent().tail = trimmed_text\n",
    "\n",
    "    if tree_root is not None:\n",
    "        text_nodes = get_body_text_nodes(tree_root)\n",
    "      \n",
    "    return text_nodes\n",
    "\n",
    "def assign_types(text_nodes):\n",
    "    def _get_ancestors(node):\n",
    "        parent = node.getparent()\n",
    "        parent_ancestors = tuple(parent.iterancestors())\n",
    "\n",
    "        ancestors = (parent,)+parent_ancestors if node.is_text else parent_ancestors\n",
    "        return ancestors\n",
    "\n",
    "    namespace_regex = re.compile('\\{.*\\}')\n",
    "    remove_namespace = lambda namespace: namespace_regex.sub('', namespace)\n",
    "\n",
    "    get_types = lambda node: tuple(remove_namespace(anc.tag) for anc in _get_ancestors(node))\n",
    "\n",
    "    return ((node, get_types(node)) for node in text_nodes)\n",
    "\n",
    "def remove_irrelevant_tags(text_nodes, irrelevant_tags):\n",
    "    return ((node, tuple(tag for tag in tags if tag not in irrelevant_tags)) for node,tags in text_nodes)\n",
    "\n",
    "def remove_irrelevant_tei_tags(text_nodes): \n",
    "    return remove_irrelevant_tags(text_nodes, \n",
    "            ('unclear', 'TEI', 'body', 'text', 'damage', 'add', 'supplied', 'div', 'span', 'p', 'del', 'note'))\n",
    "\n",
    "def assign_entity_2_fragments(text_nodes, tei_2_entity):\n",
    "    data = []\n",
    "    for fragment in text_nodes:\n",
    "        tags = fragment[1]\n",
    "        entity = 'text'\n",
    "        for tag in tags:\n",
    "            if tag in tei_2_entity:\n",
    "                entity = tei_2_entity[tag]\n",
    "                break\n",
    "    \n",
    "        data.append((fragment[0], entity))\n",
    "\n",
    "    return data\n",
    "\n",
    "def apply_tokenizer(text_nodes, tokenizer):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for fragment in text_nodes:\n",
    "        fragment_tokens = tokenizer(fragment[0])\n",
    "        tokens.extend(fragment_tokens)\n",
    "        tags.extend(tuple(fragment[1] for _ in range(len(fragment_tokens))))\n",
    "\n",
    "    return tokens, tags\n",
    "\n",
    "def apply_str_tokenizer(text_nodes):\n",
    "    return apply_tokenizer(text_nodes, str)\n",
    "\n",
    "def filter_out_annotated(text_nodes):\n",
    "        return filter(lambda text_node: len(text_node[1]) == 0, text_nodes)\n",
    "    \n",
    "def filter_out_non_annotated(text_nodes):\n",
    "        return filter(lambda text_node: len(text_node[1]) != 0, text_nodes)\n",
    "    \n",
    "def clean_spaces(text):\n",
    "    sanityzed = text.replace('\\n', ' ')\n",
    "    sanityzed = sanityzed.replace('\\r', ' ')\n",
    "    return re.sub('[ ]+', ' ', sanityzed)\n",
    "\n",
    "def extract_text_nodes(xml_tree):\n",
    "    processing = (\n",
    "        get_body_text_nodes, \n",
    "        filter_out_empty_text_nodes,\n",
    "        assign_types,\n",
    "        remove_irrelevant_tei_tags,\n",
    "        filter_out_annotated,\n",
    "        lambda text_nodes: map(lambda node: node[0], text_nodes),\n",
    "        list\n",
    "    )\n",
    "    \n",
    "    return reduce(lambda x,f: f(x), processing, xml_tree)\n",
    "\n",
    "def extract_annotations(xml_tree):\n",
    "    processing = (\n",
    "        get_body_text_nodes, \n",
    "        filter_out_empty_text_nodes,\n",
    "        assign_types,\n",
    "        remove_irrelevant_tei_tags,\n",
    "        filter_out_non_annotated,\n",
    "        tuple\n",
    "    )\n",
    "    \n",
    "    return reduce(lambda x,f: f(x), processing, xml_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_text):\n",
    "    return extract_annotations(preprocess(file_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gazeette(processed_dep_folder):\n",
    "    processed_depositions = os.listdir(processed_dep_folder)\n",
    "    processed = ()\n",
    "    for dep_name in tqdm.tqdm(processed_depositions):\n",
    "        with open(os.path.join(processed_dep_folder, dep_name)) as f:\n",
    "            new_terms = tuple(map(lambda x: (*x,dep_name), process(f.read())))\n",
    "            processed += new_terms\n",
    "    zipped = filter(lambda node: True or len(node[0]) > 0, processed)\n",
    "    gazeette = {clean_spaces(text):{'tag':tag,'origin':dep} \n",
    "                    for text, tag, dep \n",
    "                    in ((x[0], x[1][0], x[2]) \n",
    "                    for x in zipped)}\n",
    "    return gazeette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 375.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Watson\n",
      "Edward Piggot\n",
      "Nicholas Wilkins\n",
      "County of Catherlogh\n",
      "Thomas Armstrong\n",
      "James O Knae \n",
      "John Mc Owen\n",
      "Patrick Corkeran\n",
      "Joh Watson\n",
      "Marcy 1643\n",
      "Edward Piggott\n",
      "County of Carlow\n",
      "Edward Pigott\n",
      "William Hitchcock\n",
      "John Sterne\n",
      "Alice Hogg\n",
      "Barrony of Rathdowne\n",
      " of Dublin\n",
      "of October \n",
      "Murferstown\n",
      "December Last\n",
      "William Wolferston\n",
      "County of Dublin\n",
      "Luke Toole\n",
      "Countye of Wickloe, \n",
      "Banabye Toole\n",
      "Carrogroe,\n",
      "Mathew Talbott\n",
      "marke\n",
      "Deposed March 26th\n",
      "Dublin\n",
      "Octo\n",
      "Murferstowne\n",
      "Barony of Rathdowne\n",
      "of October last\n",
      "of December Last\n",
      "County of Wicklow\n",
      "Banabas Toole\n",
      "Carrogroe\n",
      "Matthew Talbot\n",
      "mark\n",
      "March 26th 1642\n",
      "March 1642\n",
      "Oct\n",
      "Henry Brereton\n",
      "Roger Didsbery\n",
      "Cittie of Dublin\n",
      "November Last\n",
      "past 1641\n",
      "Countie of Catherlogh\n",
      "Colemanaghe\n",
      "Hacketstowne\n",
      "Marcy\n",
      "City of Dublin\n",
      "Jurat March 28\n",
      "Henry Jones\n",
      "Henry Sanky\n",
      "his Maiestie\n",
      "1643\n",
      "Art Molloy\n",
      "in December 1641\n",
      "England\n",
      "Martij 1643\n",
      "Hen: Brereton\n",
      "Marcij 1643\n",
      "february Last\n",
      "1643.\n",
      "December 1641\n",
      "15 March 1643\n",
      "March 1643\n",
      "Randall Adams\n",
      "William Aldrich\n",
      "Margery Barlow\n",
      "Anthony Barlow\n",
      "county of ffermanagh\n",
      "of October 1641\n",
      "Rory Maquire\n",
      "Duncarrow Maguire\n",
      "Thomas oge Maguire\n",
      "County of ffermanagh\n",
      "Turlogh Ballagh\n",
      "1641\n",
      "John Moore\n",
      "Gabriell Gibson\n",
      "William Seaton\n",
      "three weekes\n",
      "1642\n",
      "county of\n",
      "County of fermanagh\n",
      "Randal Adams\n",
      "Roger Puttock\n",
      "Symon Swayen\n",
      "John Swayen\n",
      "Ellenor Oliverson\n",
      "Jane Evans\n",
      " december last\n",
      "December last\n",
      "Edward Barnewell\n",
      "James mc Hugh\n",
      "Shane o Lovan\n",
      "John Drake\n",
      "Hugh mcOwen\n",
      " of Wickloe\n",
      " Jane Evans\n",
      " December last\n",
      "Philip Bysse\n",
      "Ric: Williamson\n",
      "Ann Blissett\n",
      "barony of Clangibbon\n",
      "County of Corke\n",
      "Province of Munster\n",
      "of february last\n",
      "Towne of Mitchelstowne\n",
      "Martij 1642\n",
      "Philip Bisse\n",
      "Richard Williamson\n",
      "february last\n",
      "Town of Mitchelstown\n",
      "16. March 1642\n",
      "Richard Williams\n",
      "Thomas Lowe\n",
      "Jonathan Palye\n",
      "Mistris Eaton\n",
      "Leonard Graves\n",
      "Capten Bray\n",
      "Capten Clerck\n",
      "bay of Dublin,\n",
      "Owner Androes\n",
      "Mr Lin\n",
      "Gabriel Meade\n",
      "Jeremy Smith\n",
      "Tho: Coote\n",
      "Rich: Brasier\n",
      "David Gray\n",
      "March\n",
      "1652\n",
      "Allester McDonnell\n",
      "Mr Stewart\n",
      "John Mortimer\n",
      "John Spence\n",
      "Allester McNeile\n",
      "Gay Coghrane\n",
      "Robert McCurdiss\n",
      "of January 1641\n",
      "Richard Brasier\n",
      "Thomas Coote\n",
      "Tho: Dongan\n",
      "Ja Donelan\n",
      "Thomas Hooke\n",
      "Mary Carroll\n",
      "Brian Carroll\n",
      "Januar 1652 before Justice\n",
      "Counties of dublin\n",
      "of oct 1641\n",
      "Job Ward\n",
      "Edward Sneape\n",
      "Phelym mc Redmond\n",
      "James Donelan\n",
      "Thomas Dongan\n",
      "County of Kilkenny\n",
      "of January 1652\n",
      "James Donellan\n",
      "Thomas Dungan\n",
      "Donnell crone\n",
      "of March 1652\n",
      "ffergus McDowgall\n",
      "Archibald Stewart\n",
      "George Canning\n",
      "Manus roe\n",
      "Brittish Companies\n",
      "Capten Kennadaie\n",
      "of January\n",
      "James McColl McDonnell\n",
      "Allester McColl\n",
      "Donnell gorme McDonnell\n",
      "Randell McDonnell\n",
      "Brittish\n",
      "James McColl\n",
      "Scotland\n",
      "John Mortimer \n",
      "Towne of Ballymoney\n",
      "County of Antrim\n",
      "Town of Ballymoney\n",
      "Mary Goodman\n",
      "James Goodman\n",
      "County of Mayo\n",
      "Countie of Mayo\n",
      "Edmund Grane Bourk\n",
      "Patrick Joice\n",
      "Phillip Boyd\n",
      "Richard o Monelly\n",
      "Richard Barrett\n",
      "Joine mc Joine\n",
      "Richard Reaghe\n",
      "Countie of Mayoe\n",
      "September 1642\n",
      "County of downe\n",
      "6 September 1642\n",
      "Tho: Hooke\n",
      "Andrew Gyraght\n",
      "ffebruary 1652\n",
      "about July 1650\n",
      "Tady Elllis\n",
      "Patricke Keregan \n",
      " Richard Strong\n",
      "Robert Boylan\n",
      "towne of Barbistowne\n",
      "towne of the ffurrowes\n",
      " Capten Blundell\n",
      "Justice Donellan\n",
      "february 1652\n",
      "town of Barberstown\n",
      "town of the furroughs\n",
      "Keane O Hara\n",
      "of November 1641\n",
      "Phelym O Neill\n",
      "Michell Doyne\n",
      "Capten Hamilton\n",
      "Collonell Chichester\n",
      "Walter Bethell\n",
      "Tirlogh O Boghuie\n",
      "Murogh Doogh O Neill\n",
      "Neill McCan\n",
      "Donnell McCan\n",
      "Owen Savadge\n",
      "Owen O Cassidy\n",
      "Michall Poole\n",
      "Rory o Cane\n",
      "Mr Abreylow\n",
      "Tirlogh Begluie\n",
      "Humphrey Sexton\n",
      "Walter Bethell \n",
      " Tirlogh O Boghuie\n",
      "Owen Savadge \n",
      "Owen O Cassidy \n",
      "Shane O Quyn\n",
      "Robert Osborne\n",
      "Countie of Lowth\n",
      "ffrancis Wise\n",
      "Countie of\n",
      "William More\n",
      "Brian\n",
      "Countie of Monoghan\n",
      "John Taafe\n",
      "County of <strike>Meath</strike>\n",
      "Patrick Taaffe\n",
      "County of Lowth\n",
      "Robert Taafe\n",
      "Lawrence Taafe\n",
      "James Taafe\n",
      "John ffitzgerrald\n",
      "County of Waterford\n",
      "James ffennell\n",
      "6 or 7 weekes\n",
      "Countie of Lowth:\n",
      "Spaine\n",
      "Mathew pentheny\n",
      "Garrett Cooley\n",
      "13o december 1642\n",
      "13o dec 1642\n",
      "Countie of Waterford\n",
      "Owen McBrian McMaghon\n",
      " John Taafe\n",
      "County of\n",
      " John ffitzgerrald\n",
      " Mathew pentheny\n",
      "Garrett Cooley \n",
      "Christmas\n",
      "13 december 1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gazeette = create_gazeette(seed_dep_folder)\n",
    "gazeette_ = sorted(gazeette.items(), key=lambda a: -len(a[0].split()))\n",
    "for x in gazeette.keys():print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Text Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityze_text_content(text):\n",
    "    sanityzed = text.replace('<', '_')\n",
    "    sanityzed = sanityzed.replace('>', '_')\n",
    "    return sanityzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_match_certainty_tag(target, match, gazeette_term, origin):\n",
    "    \n",
    "    similarity_sum = reduce(lambda ac,dc: ac+dc[0] if dc[1] else ac, \n",
    "                            zip(match['similarity'],match['matches']), \n",
    "                            0)\n",
    "    cert = similarity_sum/match['number_matches']\n",
    "    deg = reduce(lambda ac,dc: ac*max(dc[0],0.12), \n",
    "                            zip(match['similarity'],match['matches']), \n",
    "                            1)\n",
    "    \n",
    "    desc = f'Automatically annotated based on the initial annotation \\'{sanityze_text_content(gazeette_term)}\\' ' \\\n",
    "            +f'from the document \\'{origin}\\'.\\n' \\\n",
    "            +f'{match[\"number_matches\"]} out of {len(gazeette_term.split())} highly coincident terms.\\n' \\\n",
    "            +f'Normalized Levenshtein similarity between terms: {match[\"similarity\"]}.' \n",
    "        \n",
    "    text = f'<certainty category=\"incompletness\" locus=\"name\" degree=\"{deg}\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#{target}\" desc=\"{desc}\"/>'\n",
    "    return et.fromstring(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<certainty category=\"incompletness\" locus=\"name\" degree=\"0.5357142857142857\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#place0001\" desc=\"Automatically annotated based on the initial annotation 'Edward Piggot' from the document 'own.xml'. 1 out of 2 highly coincident terms. Normalized Levenshtein similarity between terms: (0.7142857142857143, 0.75).\"/>\n"
     ]
    }
   ],
   "source": [
    "match = {'match': True,\n",
    "         'similarity': (0.7142857142857143, 0.75),\n",
    "         'matches': (False, True),\n",
    "         'number_matches': 1}\n",
    "e = create_match_certainty_tag('place0001', match, 'Edward Piggot', 'own.xml')\n",
    "print(et.tounicode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5249999999999999"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.75*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<certainty category=\"incompletness\" locus=\"name\" degree=\"0.5357142857142857\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#place0001\" desc=\"Automatically annotated based on the initial annotation 'Edward Piggot' from the document 'own.xml'. 1 out of 2 highly coincident terms. Normalized Levenshtein similarity between terms: (0.7142857142857143, 0.75).\"/>\n"
     ]
    }
   ],
   "source": [
    "match = {'match': True,\n",
    "         'similarity': (0.7142857142857143, 0.75),\n",
    "         'matches': (False, True),\n",
    "         'number_matches': 1}\n",
    "e = create_match_certainty_tag('place0001', match, 'Edward Piggot', 'own.xml')\n",
    "print(et.tounicode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {'match': True,\n",
    "         'similarity': (0.7142857142857143, 0.75),\n",
    "         'matches': (False, True),\n",
    "         'number_matches': 1}\n",
    "e = create_match_certainty_tag('place0001', match, 'Edward Piggot', 'own.xml')\n",
    "print(et.tounicode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<certainty category=\"incompletness\" locus=\"name\" degree=\"0.8\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#date0001\" desc=\"Automatically annotated based on high confidence in date match (march). Exact month match.\"/>\n"
     ]
    }
   ],
   "source": [
    "def create_time_certainty_tag(target, gazeette_term):    \n",
    "    desc = f'Automatically annotated based on high confidence in date match ({sanityze_text_content(gazeette_term)}). Exact month match.' \n",
    "        \n",
    "    text = f'<certainty category=\"incompletness\" locus=\"name\" degree=\"0.8\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#{target}\" desc=\"{desc}\"/>'\n",
    "    return et.fromstring(text)\n",
    "\n",
    "e = create_time_certainty_tag('date0001', 'march')\n",
    "print(et.tounicode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<certainty category=\"incompletness\" locus=\"name\" degree=\"0.3\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#date0001\" desc=\"Automatically annotated based on low confidence in date match (march). High similarity with short length annotations.\"/>\n"
     ]
    }
   ],
   "source": [
    "def create_year_certainty_tag(target, gazeette_term):    \n",
    "    desc = f'Automatically annotated based on low confidence in date match ({sanityze_text_content(gazeette_term)}). High similarity with short length annotations.' \n",
    "        \n",
    "    text = f'<certainty category=\"incompletness\" locus=\"name\" degree=\"0.3\" cert=\"unknown\" resp=\"#automatic_gazeetter_ner\" target=\"#{target}\" desc=\"{desc}\"/>'\n",
    "    return et.fromstring(text)\n",
    "\n",
    "e = create_year_certainty_tag('date0001', 'march')\n",
    "print(et.tounicode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_certainty(dep_tree, certainty):\n",
    "    certainties = dep_tree.xpath('//tei:teiHeader'\n",
    "                                 '//tei:classCode[@scheme=\"http://providedh.eu/uncertainty/ns/1.0\"]',\n",
    "                                 namespaces=namespaces)\n",
    "\n",
    "    certainties[0].append(certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagIds:\n",
    "    def __init__(self):\n",
    "        self._counters = {}\n",
    "    def next_id(self, tag):\n",
    "        if tag in self._counters:\n",
    "            return f'm_{tag}{next(self._counters[tag]):04}'\n",
    "        else:\n",
    "            self._counters[tag] = itertools.count()\n",
    "            return f'm_{tag}{0:04}'\n",
    "tag_ids = TagIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_in_tag(tag_id, text,substring,tag_name):\n",
    "    text_node = text.getparent()\n",
    "    parent = text_node.getparent()\n",
    "    unprocessed_leaf_nodes: tuple\n",
    "    partitions = text.partition(substring)\n",
    "    \n",
    "    new_element = et.fromstring(f'<{tag_name} xml:id=\"{tag_id}\"></{tag_name}>')\n",
    "    new_element.text = partitions[1]\n",
    "    new_element.tail = partitions[2]\n",
    "    \n",
    "    new_element_tail = new_element.xpath(\"//text()\")[1]\n",
    "        \n",
    "    if text.is_text:\n",
    "        text_node.text = partitions[0]\n",
    "        text_node.insert(0, new_element)\n",
    "        \n",
    "        parent_text = text_node.xpath('.//text()')[0]     \n",
    "        unprocessed_leaf_nodes = (parent_text, new_element_tail)\n",
    "        \n",
    "    elif text.is_tail:\n",
    "        index = parent.index(text_node) + 1\n",
    "        text_node.tail = partitions[0]\n",
    "        parent.insert(index,new_element)\n",
    "        \n",
    "        parent_tail = None\n",
    "        for x in parent.getparent().xpath('.//text()'):\n",
    "            if x.getparent() == parent and x.is_tail == True:\n",
    "                parent_tail = x\n",
    "        unprocessed_leaf_nodes = (parent_tail, new_element_tail)\n",
    "        \n",
    "    return unprocessed_leaf_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    of Dublin Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    of <place xml:id=\"a0\">Dublin</place> Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "Textual context of the new tag > ('\\n', ' Doctor in Divinity being duely\\n')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    He,\\n    <person>\\n    Alex\\n    </person>\\n    of Dublin Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    He,\\n    <person>\\n    Alex\\n    </person>\\n    of <place xml:id=\"a0\">Dublin</place> Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "Textual context of the new tag > ('\\n', ' Doctor in Divinity being duely\\n')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    of Dublin Doctor\\n    <person>\\n    Alex\\n    </person>\\n    in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    of <place xml:id=\"a0\">Dublin</place> Doctor\\n    <person>\\n    Alex\\n    </person>\\n    in Divinity being duely\\n</p>\\n</span>'\n",
      "Textual context of the new tag > ('\\n    of ', ' Doctor\\n    ')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    from\\n    <person>\\n    Alex\\n    </person>\\n    Dublin <date>Doctor</date> in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    from\\n    <person>\\n    Alex\\n    </person>\\n    <place xml:id=\"a0\">Dublin</place> <date>Doctor</date> in Divinity being duely\\n</p>\\n</span>'\n",
      "Textual context of the new tag > ('\\n', ' ')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    Doctor in Divinity being of Dublin\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    Doctor in Divinity being of <place xml:id=\"a0\">Dublin</place>\\n</p>\\n</span>'\n",
      "Textual context of the new tag > ('\\n', '\\n')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex from Dublin\\n    </person>\\n    of in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex from <place xml:id=\"a0\">Dublin</place>\\n    </person>\\n    of in Divinity being duely\\n</p>\\n</span>'\n",
      "Textual context of the new tag > ('\\n    Alex from ', '\\n    ')\n"
     ]
    }
   ],
   "source": [
    "a = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    of Dublin Doctor in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "a_ = a.xpath('//text()')\n",
    "\n",
    "b = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    He,\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    of Dublin Doctor in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "b_ = b.xpath('//text()')\n",
    "\n",
    "c = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    of Dublin Doctor\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "c_ = c.xpath('//text()')\n",
    "\n",
    "d = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    from\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    Dublin <date>Doctor</date> in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "d_ = d.xpath('//text()')\n",
    "\n",
    "e = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    Doctor in Divinity being of Dublin\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "e_ = e.xpath('//text()')\n",
    "\n",
    "f = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    <person>\n",
    "    Alex from Dublin\n",
    "    </person>\n",
    "    of in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "f_ = f.xpath('//text()')\n",
    "\n",
    "print(et.tostring(a))\n",
    "leafs = wrap_text_in_tag('a0', a_[3], 'Dublin', 'place')\n",
    "print(et.tostring(a))\n",
    "print('Textual context of the new tag >',leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(b))\n",
    "leafs = wrap_text_in_tag('a0', b_[3], 'Dublin', 'place')\n",
    "print(et.tostring(b))\n",
    "print('Textual context of the new tag >',leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(c))\n",
    "leafs = wrap_text_in_tag('a0', c_[1], 'Dublin', 'place')\n",
    "print(et.tostring(c))\n",
    "print('Textual context of the new tag >',leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(d))\n",
    "leafs = wrap_text_in_tag('a0', d_[3], 'Dublin', 'place')\n",
    "print(et.tostring(d))\n",
    "print('Textual context of the new tag >',leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(e))\n",
    "leafs = wrap_text_in_tag('a0', e_[3], 'Dublin', 'place')\n",
    "print(et.tostring(e))\n",
    "print('Textual context of the new tag >',leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(f))\n",
    "leafs = wrap_text_in_tag('a0', f_[2], 'Dublin', 'place')\n",
    "print(et.tostring(f))\n",
    "print('Textual context of the new tag >',leafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. String Matching for Gazeette entries\n",
    "\n",
    "The distance of each word of an n-gram to the correspondent of the gazeette entry is meassured using the Normalized Levenshtein distance.\n",
    "\n",
    "__The Normalized Levenshtein distance is not a metric__: Therefore the triangle inequality does not apply; which\n",
    "allow us to sum the distances of each word and not take order into account.\n",
    "\n",
    "__Given the high degree of variability in named entties and the low coverage of annotated text__, matching is based on the amount of matches for a given n-gram instead of relying solely on the distance of the complete text fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_levenshtein = NormalizedLevenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_similarity = 0.6\n",
    "min_n_gram_similarity = 2/3\n",
    "\n",
    "def test_match_(text1, text2, min_word_similarity, min_n_gram_similarity):\n",
    "    similarity = tuple(normalized_levenshtein.similarity(x.lower(), y.lower()) for x,y in zip(text1, text2))\n",
    "    \n",
    "    matching = tuple(map(lambda x: x>=min_word_similarity, similarity))\n",
    "    match_n = reduce(lambda ac,dc: ac+1 if dc else ac, matching, 0)\n",
    "    \n",
    "    match = {\n",
    "        'match': match_n/len(text1) >= min_n_gram_similarity,\n",
    "        'similarity': similarity,\n",
    "        'matches': matching,\n",
    "        'number_matches': match_n\n",
    "    }\n",
    "    \n",
    "    if len(text1) == 2 and (len(text1) < 4 or len(text2) < 4):\n",
    "        match['match'] = False\n",
    "    \n",
    "    return match\n",
    "\n",
    "def test_match(text1, text2):\n",
    "    return test_match_(text1, text2, min_word_similarity, min_n_gram_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match': True,\n",
       " 'similarity': (0.7142857142857143, 1.0, 0.75),\n",
       " 'matches': (False, True, True),\n",
       " 'number_matches': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match(['County','of','Gilliegh'], ['Countie','of','Gilleigh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match': True,\n",
       " 'similarity': (0.4444444444444444, 1.0, 0.75),\n",
       " 'matches': (False, True, True),\n",
       " 'number_matches': 2}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match('Patrick mc Gilliegh'.split(), 'Catherine mc Gilleigh'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Heuristic rules for identifying dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_anotate_dates(dep_text_nodes):\n",
    "    namespaces = {'tei': 'http://www.tei-c.org/ns/1.0', 'xml': 'http://www.w3.org/XML/1998/namespace'}\n",
    "    months = [\n",
    "        'january', 'jan', 'february', 'feb', 'march', \n",
    "        'mar', 'april', 'jpr', 'may', 'may', 'june', \n",
    "        'jun', 'july', 'jul', 'august', 'aug', \n",
    "        'september', 'sep', 'october', 'oct', 'november', \n",
    "        'nov', 'december', 'dec'\n",
    "    ]\n",
    "    \n",
    "    while len(dep_text_nodes) > 0:\n",
    "        node = dep_text_nodes.pop(0)\n",
    "        words = node.split(' ')\n",
    "             \n",
    "        if len(words) < 3:\n",
    "            pass\n",
    "                                      \n",
    "        text_len = 3 \n",
    "                                      \n",
    "        match = False\n",
    "        for i in range(0, len(words) + 1 - text_len):\n",
    "            matching_text = ' '.join(words[i : i+text_len]).strip()\n",
    "            if words[i+1].lower() in months:\n",
    "                tag_id = tag_ids.next_id('date')\n",
    "                left_nodes = wrap_text_in_tag(tag_id, node, matching_text, 'date')\n",
    "                e = create_time_certainty_tag(tag_id, words[i+1])\n",
    "                add_certainty(node.getparent().getroottree(), e)\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gazeette(dep_text_nodes, gazeette):   \n",
    "    gazeette_sorted = sorted(gazeette.items(), key=lambda a: -len(a[0]))\n",
    "    annotated = 0\n",
    "    while len(dep_text_nodes) > 0:\n",
    "        node = dep_text_nodes.pop(0)\n",
    "        words = node.split(' ')\n",
    "            \n",
    "        for entry in gazeette_sorted:\n",
    "            text_len = len(entry[0].split(' '))\n",
    "                \n",
    "            if len(words) < text_len:\n",
    "                pass\n",
    "            \n",
    "            match = False\n",
    "            for i in range(0, len(words) + 1 - text_len):\n",
    "                if i >= len(words) or (i + text_len) >= len(words):\n",
    "                    break\n",
    "                    \n",
    "                matching_text = (words[i],) if text_len == 1 else words[i : i+text_len]\n",
    "                gazeette_text = (entry[0],) if text_len == 1 else entry[0].split()\n",
    "                \n",
    "                matching = test_match(matching_text, gazeette_text)\n",
    "                \n",
    "                if matching['match']:\n",
    "                    tag_id = tag_ids.next_id(entry[1]['tag'])\n",
    "                    left_nodes = wrap_text_in_tag(tag_id, node, ' '.join(matching_text), entry[1]['tag'])\n",
    "                    e = create_match_certainty_tag(tag_id, matching, entry[0], entry[1]['origin'])\n",
    "                    add_certainty(node.getparent().getroottree(), e)\n",
    "                    \n",
    "                    dep_text_nodes.insert(0, left_nodes[1])\n",
    "                    dep_text_nodes.insert(0, left_nodes[0])\n",
    "                    match = True\n",
    "                    annotated += 1\n",
    "                    break\n",
    "                    \n",
    "            if match:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_annotation_date_processing(dep_text_nodes): \n",
    "    while len(dep_text_nodes) > 0:\n",
    "        node = dep_text_nodes.pop(0)\n",
    "        words = node.split(' ')\n",
    "             \n",
    "        if len(words) < 4:\n",
    "            pass\n",
    "                                      \n",
    "        text_len = 4 # dates\n",
    "                                      \n",
    "        match = False\n",
    "        for i in range(0, len(words) + 1 - text_len):\n",
    "            matching_text = ' '.join(words[i : i+text_len]).strip()\n",
    "                \n",
    "            if re.match('[0-9]{4}', words[i+1].lower()):\n",
    "                tag_id = tag_ids.next_id('date')\n",
    "                left_nodes = wrap_text_in_tag(tag_id, node, matching_text, 'date')\n",
    "                e = create_year_certainty_tag(tag_id, words[i+1])\n",
    "                add_certainty(node.getparent().getroottree(), e)\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(dep_folder, dep_name, gazeette):\n",
    "    with open(os.path.join(dep_folder, dep_name)) as f:\n",
    "        dep_raw = f.read()\n",
    "        \n",
    "    dep_tree = preprocess(dep_raw)\n",
    "    \n",
    "    apply_gazeette(extract_text_nodes(dep_tree), gazeette)\n",
    "    pre_anotate_dates(extract_text_nodes(dep_tree))\n",
    "    post_annotation_date_processing(extract_text_nodes(dep_tree))\n",
    "    \n",
    "    return dep_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Apply Gazeette to Depositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original_normalized_depositions_marked_persons.log'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(original_dep_folder)[2928]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = 891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(depositions_dir, processed_dir, dep_name, gazeette):\n",
    "    try:\n",
    "        if Path(os.path.join(depositions_dir, dep_name)).is_file():\n",
    "            applied = annotate(depositions_dir, dep_name, gazeette)\n",
    "            \n",
    "            with open(os.path.join(processed_dir, dep_name), 'w') as f:\n",
    "                f.write(et.tostring(applied).decode('UTF-8'))\n",
    "    except Exception as e:\n",
    "        pass #print(e)\n",
    "def apply_ner(dep_name):\n",
    "    f(original_dep_folder, processed_dep_folder, dep_name, gazeette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1008/1008 [19:41<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "num_proc = 8\n",
    "dep_i_start = 6000\n",
    "dep_i_end = len(depositions)\n",
    "tqdm.tqdm._instances.clear()\n",
    "with ProcessPoolExecutor(max_workers=num_proc) as pool:\n",
    "    futures = tuple(pool.submit(apply_ner, dep_name) for dep_name in depositions[dep_i_start:dep_i_end])\n",
    "    kwargs = {\n",
    "        'total': len(futures),\n",
    "    }\n",
    "    for _ in tqdm.tqdm(as_completed(futures), **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12:20 el último revisado manualmente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
